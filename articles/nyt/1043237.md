<!--1702026421000-->
[AI毁灭人类的“末日概率”是多少](https://cn.nytimes.com/technology/20231208/silicon-valley-artificial-intelligence/)
------

<address>KEVIN ROOSE</address><time pudate="2023-12-08 04:45:49" datetime="2023-12-08 04:45:49">2023年12月8日</time><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/12/07/multimedia/07sp-dealbook-advance-AI-lzqp/07sp-dealbook-advance-AI-lzqp-master1050.jpg" width="1050" height="700"><figcaption>达里奥·阿莫代伊展示如何用简单的电子游戏训练AI机器人。 <cite>Christie Hemm Klok for The New York Times</cite></figcaption></figure><section><p>本文是“<a href="https://www.nytimes.com/spotlight/dealbook-special-section">交易录峰会”(DealBook Summit)</a>特别报道的一部分，该峰会得到了世界各地商业及政策领袖的参与。</p><p>AI公司<a rel="noopener noreferrer" target="_blank" href="https://www.anthropic.com/" title="Link: https://www.anthropic.com/">Anthropic</a>的首席执行官达里奥·阿莫代伊<a rel="noopener noreferrer" target="_blank" href="https://twitter.com/liron/status/1710520914444718459">给出</a>的数字是10%到25%之间。联邦贸易委员会主席丽娜·汗近日告诉我，她认为的数值是15%。上月担任了五分钟的OpenAI临时首席执行官的艾米特·席尔<a href="https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html" title="Link: https://www.nytimes.com/2023/11/10/podcasts/hardfork-chatbot-ftc.html">告诉我</a>，他得出的数值<a rel="noopener noreferrer" target="_blank" href="https://www.huffpost.com/entry/openai-doom-warning_n_655b488be4b0ce6a31bde4a5">游移</a>在5%到50%之间。</p><p>我这里说的，当然就是p(末日)了，这是硅谷正在热议的一项令人毛骨悚然的新数据。</p><p>P(末日)——就是用数学的方式表达“末日的概率”——是一些人工智能研究者在表达他们认为AI有多大可能把我们杀光，或创造别的什么威胁人类生存的灾变。P(末日)高意味着你认为AI末日的可能性大，低意味着你认为我们有机会挺过去。</p><p>P(末日)曾经只是网络论坛上一帮AI技术宅之间心领神会的笑话，但随着去年ChatGPT点燃的AI热让人对AI的进步之快产生恐惧，近几个月这个词进入了主流。</p><p>它已经成了旧金山技术人士交谈时常用的开场白——也是AI文化里躲不开的一部分。今年我在两场业界活动上遇到陌生人问我的p(末日)是多少，就像在问洗手间怎么走一样随意。“几乎每一场晚餐谈话都会提起，”<a rel="noopener noreferrer" target="_blank" href="https://www.box.com/">云数据平台Box</a>首席执行官艾伦·勒维对我说。</p><p>P(末日)甚至在上月OpenAI事变中对扮演了一定的角色。席尔被任命为OpenAI临时负责人后，公司雇员之间开始转发他在近日一期博客上的言论，称他的p(末日)可能高达50%。一位讨论时在场的人士说，有员工担心他是个“末日论者”，由于认为这太过冒险，他可能会寻求延缓或限制他们的工作。（被罢免的OpenAI首席执行官萨姆·奥尔特曼最终恢复原职，因此这件事也就无关紧要了。）</p><p>当然，科幻迷很久以前就已经开始推想机器人篡夺世界的景象。但在去年ChatGPT推出后，这种威胁似乎显得更真切了。毕竟如果AI模型可以<a href="https://www.nytimes.com/2022/09/02/technology/ai-artificial-intelligence-artists.html">赢得艺术奖项</a>，<a rel="noopener noreferrer" target="_blank" href="https://law.stanford.edu/2023/04/19/gpt-4-passes-the-bar-exam-what-that-means-for-artificial-intelligence-tools-in-the-legal-industry/">通过律师资格考试</a>，那么距离劫难还能有多远？</p><p><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/12/07/multimedia/07sp-dealbook-advance-AI-vlfz/07sp-dealbook-advance-AI-vlfz-jumbo.jpg"></p><figcaption>人工智能公司Anthropic联合创始人、首席执行官达里奥·阿莫代伊。 <cite>Massimo Berruti for The New York Times</cite></figcaption></figure><p>AI行内人也在发出警告。去年<a href="https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html">从谷歌辞职</a>的著名AI研究者杰弗里·辛顿开始警告AI的风险，他近日<a rel="noopener noreferrer" target="_blank" href="https://twitter.com/geoffreyhinton/status/1719447980753719543">估计</a>如果不施加有力的监管，AI在未来30年导致人类灭绝的可能性为10%。和辛顿一同被誉为“深度学习教父”的约书亚·本吉奥<a rel="noopener noreferrer" target="_blank" href="https://www.abc.net.au/news/2023-07-15/whats-your-pdoom-ai-researchers-worry-catastrophe/102591340">在接受采访时说</a>，他认为发生AI大劫难的可能性大约在20%。</p><p>没人知道我们被AI杀死的概率是10%、20%还是85.2%。显然这个问题会引出更多的问题，比如：如果AI导致50%的人类死亡，还算“末日”吗？如果没有人死，但我们全都丢了工作，只能喝西北风呢？到底AI会如何接管世界呢？</p><p>但是p(末日)的重点不在于精确。它的目的是表明，在是乌托邦还是反乌托邦的问题上，一个人大致的立场如何，此外也是用模糊的经验论术语传达你在AI问题及其潜在影响上的关切程度。</p><p>这个术语似乎是十多年前在网络论坛<a rel="noopener noreferrer" target="_blank" href="https://www.lesswrong.com/">LessWrong</a>上出现的，那是一个理性主义哲学运动主题论坛。</p><p>LessWrong创始人是一位自学成才的AI研究者，名叫埃利泽·尤科夫斯基，他很早就产生了失控AI可能接管一切的想法，并就他设想的多种AI灾难场景撰文。（当时的AI连设置个厨房计时器都很勉强，因此风险显得十分遥远。）</p><p>后来成了AI世界<a rel="noopener noreferrer" target="_blank" href="https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/">最著名末日论者</a>的尤科夫斯基告诉我，p(末日)这个词不是他提出的，不过他做了推广普及。（他还说，如果继续以现在的趋势发展，他的p(末日)是“有多高就多高”。）这个词后来被<a rel="noopener noreferrer" target="_blank" href="https://www.effectivealtruism.org/">有效利他主义者们</a>拿去用了，这个群体旨在通过逻辑推论得出有关公序良俗的理念。</p><p>我的猜测是，这个词最早是由身在波士顿的蒂姆·泰勒<a rel="noopener noreferrer" target="_blank" href="https://twitter.com/tim_tyler/status/1665571547111649286">于2009年开始</a>在LessWrong上使用的。泰勒在邮件往来中说到，他用这个词“指代末日出现的概率，同时又不需要详细阐明时间尺度或末日的定义。”</p><p>对一些人来说，谈论你的p(末日)无非就是在闲扯。然而在硅谷的一场激烈辩论中，它又成了一个重要的社交信号，辩论的一方认为AI发展过快，另一方认为应该更快。</p><p><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/12/07/multimedia/07sp-dealbook-advance-AI-ckth/07sp-dealbook-advance-AI-ckth-master1050.jpg"><small style="color: #999;">4月，英国出生的计算机科学家杰弗里·辛顿在多伦多家中。</small></p><p>Box首席执行官勒维属于较乐观的一方。他说他的p(末日)值很低——不是零，但“能多低就多低”——他认为我们能化解AI的巨大风险，避免最糟糕的结果。他担心的不是AI会杀死我们，而是监管者和立法者会利用这些可怕的末日预言，用来理直气壮地打压一个前景远大的年轻行业。</p><p>“如果在AI发展中过早开始关键的政策决断，是有可能出现过度干预的，”他说。</p><p>P(末日)的另一个问题是，在关乎存亡的问题上，乐观与悲观的界线是不明确的。比如，如果你预言AI有15%的可能杀死所有人类，你真的算AI乐观主义者？（换个说法：如果你认为你下一次乘坐飞机“只有”15%的几率机毁人亡，你会上飞机吗？）</p><p>研究AI风险的<a rel="noopener noreferrer" target="_blank" href="https://www.openphilanthropy.org/" title="Link: https://www.openphilanthropy.org/">Open Philanthropy</a>高级研究员阿杰亚·科特拉在p(末日)的问题上花了不少时间。她认为它作为一个简称可能是有用的——顺便一提，她的p(末日)是20%到30%之间——但同时也存在局限。首先p(末日)没有考虑到，AI相关破坏的可能性在很大程度上取决于我们打算如何治理它。</p><p>“我知道有些人的p(末日)超过90%了，这在一定程度上是因为他们认为企业和政府不会操心什么良好的安全实践和政策措施，”她对我说。“我也知道有些人的p(末日)不到5%，这部分是因为他们预计科学家和政策制定者会努力预防灾难的发生。”</p><p>换句话说，你可以把p(末日)当做某种墨迹测验——这个数据说的是AI，但说到底更多是在表明我们是怎么看待人类的，我们能不能在控制住风险的同时让强大的新技术为我们所用。</p><p>那么，你的p(末日)是多少？</p></section><footer><p>Kevin Roose是《纽约时报》科技专栏作家和播客“Hard Fork”的主持人。<a rel="nofollow" target="_blank" href="https://www.nytimes.com/by/kevin-roose">点击查看更多关于他的信息。</a></p><p>翻译：杜然</p><p><a rel="nofollow" target="_blank" href="https://www.nytimes.com/2023/12/06/business/dealbook/silicon-valley-artificial-intelligence.html">点击查看本文英文版。</a></p></footer>
