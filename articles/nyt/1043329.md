<!--1699432621000-->
[政府究竟该如何监管人工智能](https://cn.nytimes.com/opinion/20231108/biden-ai-regulation/)
------

<address>吴修铭</address><time pudate="2023-11-08 04:05:17" datetime="2023-11-08 04:05:17">2023年11月8日</time><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/11/07/opinion/07wu-image/07wu-image-master1050.jpg" width="1050" height="1050"><figcaption> <cite>Illustration by Sam Whitney/The New York Times</cite></figcaption></figure><section><p>上周，在<a href="https://www.nytimes.com/2023/10/30/us/politics/biden-ai-regulation.html">签署</a>关于人工智能的全面行政命令时，拜登总统开玩笑说，看到“深度伪造”的自己是一种奇怪的体验，“我到底什么时候说过那种话？”</p><p>他的调侃意味深长，将这项行政命令与一种人人都能理解的人工智能危害联系了起来，那就是冒充他人。另一个案例是近来泛滥的虚拟裸照，已经<a rel="noopener noreferrer" target="_blank" href="https://www.wsj.com/tech/fake-nudes-of-real-students-cause-an-uproar-at-a-new-jersey-high-school-df10f1bb">毁掉了</a>许多<a rel="noopener noreferrer" target="_blank" href="https://www.washingtonpost.com/technology/2023/11/05/ai-deepfake-porn-teens-women-impact/">高中女孩</a>的生活。这些已成为日常的事件都指向一个重要的事实，那就是监管人工智能的努力是否成功，将决定政府能否专注于解决诸如深度伪造的切实问题，而不是被机器主宰人类等臆想危险所淹没。</p><p>拜登的行政命令甚至超出了欧洲政府的监管范畴，从无处不在的诈骗到大规模杀伤性武器的研发，涵盖了人们能想到的几乎所有潜在风险。该命令将为人工智能安全和可信度设下标准，打造一项开发人工智能工具的网络安全计划，并要求开发可能对国家安全构成威胁的人工智能系统的企业向联邦政府分享安全测试结果。</p><p>白宫在人工智能问题上如此大力投入是正确的决定，为的是避免重蹈2010年代在有效监管社交媒体问题上的灾难性覆辙。因为政府的袖手旁观，社交媒体技术从看似人畜无害的私人朋友圈更新工具演变为大规模心理操纵的手段，加之侵犯隐私的商业模式和伤害青少年、助长错误信息并推动洗脑灌输传播的黑历史，堪称五毒俱全。</p><p>但如果说社交网络是披着羊皮的狼，那人工智能这头“狼”更像是披上了末日骑士的外衣。大众的想象总将人工智能与斯坦利·库布里克的电影《2001太空漫游》(2001: A Space Odyssey)中发生邪恶故障的“哈尔”(HAL 9000)，以及《终结者》(Terminator)系列中拥有自我意识的大反派“天网”(Skynet)联系到一起。不过，虽然人工智能带来的问题和挑战确实需要政府采取行动，但对末日到来的担忧——无论是自动化带来的大规模失业还是以灭绝人类为使命的超智慧人工智能——仍然只是猜测。</p><p>如果说政府对社交媒体作为太少且太迟是个错误，那我们如今需要警惕的是过早采取政府行动，导致实际危害难以得到解决。</p><p>忍不住想拿出过度反应是可以理解的。谁也不想成为灾难电影里那些无知的官员，对即将到来的灾变的早期征兆不屑一顾。白宫想对人工智能进行标准化测试并对灾难性风险进行独立监督没有错。拜登的行政命令要求研发最强大人工智能系统的企业向政府通报安全测试情况，并要劳工部长研究人工智能取代人类岗位的风险和补救措施。</p><p>但事实是，没人知道这些惊天动地的发展是否能够实现。科技的预言与气候科学不同，其参数是相对有限的。从每周30小时和15小时的工作时间到电视的消亡，科技的历史中充斥着始终没有发生的自信预期和“必然”。以沉重口吻论述令人恐惧的可能性确实能吸引眼球。但这也是世界为应对“千年虫”而浪费数以千亿计美元的原因。</p><p>监管充满变数的风险而非实际危害的做法并不明智，原因有二。首先，过于急切的监管者可能会目光短浅地锁定错误的监管对象。例如，为了应对数字盗版风险，国会曾于1992年对数码录音带进行广泛监管，但随着互联网和MP3的崛起，如今只有发烧友才会记得这种录音媒介。无独有偶，今天的政策制定者都在关注ChatGPT这样的大语言模型，它们是可能塑造未来——但考虑到其根植于持续<a rel="noopener noreferrer" target="_blank" href="https://www.nature.com/articles/s41537-023-00379-4">伪造和扭曲</a>的整体不可靠性，它们最终可能只会成为人工智能时代的呼啦圈。</p><p>其次，先制性监管会给有意突入某一产业的公司制造障碍。有实力的企业可以在律师和专家上花费数百万美元，想方设法适应一套复杂的新监管规则，然而规模较小的初创公司通常没有这样的资源。这就会滋养垄断，抑制创新。科技产业已经过多地被掌握在几家大公司手里。对人工智能进行最严格的监管会导致只有谷歌、微软、苹果以及它们的主要合作伙伴能在该领域竞争。最积极的AI严格监管<a rel="noopener noreferrer" target="_blank" href="https://www.geekwire.com/2023/microsoft-calls-for-safety-brakes-licensing-and-a-new-federal-agency-to-avoid-ai-pitfalls/">倡导者</a>正是这些公司和合作伙伴，这不是巧合。</p><p>相比空想出来的风险，以实际危害来引导政府在何时以何种方式介入，是好得多的标准。人工智能目前最明显的危害在于人类模仿（例如假冒的裸照）、歧视和年轻人成瘾。2020年，窃贼<a rel="noopener noreferrer" target="_blank" href="https://www.forbes.com/sites/thomasbrewster/2021/10/14/huge-bank-fraud-uses-deep-fake-voice-tech-to-steal-millions/?sh=5db45d047559">使用冒充的人声</a>从一家在香港的日本企业那里骗走了3500万美元。面部识别技术导致了<a href="https://www.nytimes.com/2020/12/29/technology/facial-recognition-misidentify-jail.html">错误的逮捕和监禁</a>，比如尼杰尔·帕克斯案，由于身份识别错误，他在新泽西州一家拘留所被关了10天。假冒的顾客评价<a rel="noopener noreferrer" target="_blank" href="https://time.com/6192933/fake-reviews-regulation/">损害了消费者信心</a>，假社媒账号在推动政治宣传。由AI驱动的算法被用于增强本已经具有成瘾性的社交媒体属性。</p><p>这些例子听上去不像人工智能安全中心<a href="https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html">今年发布</a>的警告那么惊悚，报告称“降低因人工智能导致灭绝的风险，应该是与传染病大流行和核战争等社会规模风险一样的全球优先事项”。但是这些不那么耸动的例子，偏偏是存在真实的受害者的。</p><p>值得肯定的是，拜登的行政令并没有完全受制于这样的假说：它的主要内容是为将来的行动给出一个框架。命令中的某些建议是紧迫而重要的，例如建立人工智能生成照片、视频、音频和文字水印标记的标准。</p><p>但是，行政部门的力量当然是有限的。国会应该仿效行政部门，对那些假设性问题保持关注，同时采取有力的行动，让我们免受人类模仿、算法操控、不实信息和其他紧迫的人工智能问题的危害——当然还要通过网络隐私和儿童保护法律，这些法案已在国会进行反复听证，有民众的支持，却<a rel="noopener noreferrer" target="_blank" href="https://www.theatlantic.com/technology/archive/2023/10/protect-children-online-social-media-internet/675825/">始终未能颁布</a>。</p><p>监管并不像你在程式化的政治辩论中所听到的那样，与这个或那个政党的利益保持着一致。它只是在行使政府权力，这可能是好事也可能是坏事，可能是在保护弱势者，也可能是加强已有的权力。着眼于未知未来的人工智能监管可能会被用于协助权力者确保垄断地位，拖累那些致力于用计算技术改善人类状况的人。如果能以正确的方式着眼于当下，它也许能保护弱势者，促进更广泛、更有益的创新。</p><p>具体社会危害的存在一直都是政府行为是否合理的试金石。但这是把双刃剑：政府在没有危害时应该保持警惕，并在发现危害时有责任采取行动。从这个角度讲，在人工智能的问题上，我们可能在做得过分的同时也做得不够。</p></section><footer><p>吴修铭是哥伦比亚大学法学教授，著有《The Curse of Bigness: Antitrust in the New Gilded Age》一书。</p><p>翻译：纽约时报中文网</p><p><a rel="nofollow" target="_blank" href="https://www.nytimes.com/2023/11/07/opinion/biden-ai-regulation.html">点击查看本文英文版。</a></p></footer>
