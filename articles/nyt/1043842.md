<!--1685513222000-->
[科技行业领袖警告AI可能给人类带来“灭绝风险”](https://cn.nytimes.com/technology/20230531/ai-threat-warning/)
------

<address>KEVIN ROOSE</address><time pudate="2023-05-31 01:45:00" datetime="2023-05-31 01:45:00">2023年5月31日</time><section><p>一群行业领袖周二警告，他们正在开发的人工智能技术有朝一日可能对人类的生存构成威胁，应被视为与大流行病及核战争同等的社会风险。</p><p>非营利组织人工智能安全中心发表的一份<a rel="noopener noreferrer" target="_blank" href="https://www.safe.ai/statement-on-ai-risk">只有一句话的声明</a>写道：“减轻人工智能带来的灭绝风险应该成为全球的优先事项，就像应对其他社会规模的风险——如大流行病和核战争一样。”这封公开信由逾350名从事人工智能工作的高管、研究人员和工程师签署。</p><p>签署者包括三家领先人工智能公司的高管：OpenAI首席执行官萨姆·奥尔特曼，谷歌DeepMind首席执行官杰米斯·哈萨比斯，以及Anthropic首席执行官达里奥·阿莫代伊。</p><p>杰弗里·辛顿和约书亚·本吉奥是因在神经网络方面的开创性工作而获得图灵奖的三位研究人员中的两位，他们通常被视为现代人工智能运动的“教父”，他们也签署了该声明，此外还有该领域的其他杰出研究人员。(截至周二，第三位图灵奖得主、Meta人工智能研究项目负责人扬·勒昆尚未签名。)</p><p>该声明是在人们对人工智能可能存在的危害越来越担心之际发表的。最近，所谓大型语言模型，也就是ChatGPT和其他聊天机器人使用的人工智能系统所取得的进展引发了人们的担忧——人工智能有可能很快就会被大规模用于传播错误信息和宣传，或者可能消除数以百万计白领的工作。</p><p>一些人认为，最终，人工智能可能变得足够强大，如果不采取任何措施来减缓其发展，它可能会在几年内造成社会规模的破坏，尽管研究人员有时并没有解释这种情况如何发生。</p><p>这些担忧得到了许多行业领导者的认同，这使他们处于一个不同寻常的境地，他们声称，他们正在开发的一项技术——在许多情况下，他们正在拼命工作，以图比竞争对手更快——构成了严重的风险，应该受到更严格的监管。</p><p>本月，奥尔特曼、哈萨比斯和阿莫代伊与美国总统拜登和副总统哈里斯碰面，讨论人工智能监管问题。在<a href="https://www.nytimes.com/2023/05/16/technology/openai-altman-artificial-intelligence-regulation.html">会后的参议院证词</a>中，奥尔特曼警告，先进人工智能系统的风险已经严重到需要政府干预的程度，并呼吁对人工智能的潜在危害进行监管。</p><p>人工智能安全中心执行主任丹·亨德里克斯在接受采访时表示，这封公开信代表了一些行业领袖的“出柜”，他们曾对自己正在开发的技术存在的风险表示担忧——但只是在私下里。</p><p>“即使在人工智能界，也存在普遍误解，认为只有少数人是末日论者，”亨德里克斯说。“但事实上，很多人私下里都会对这些事情表示担忧。”</p><p>一些怀疑论者认为，人工智能技术还太不成熟，不足以构成生死攸关的威胁。对于今天的人工智能系统，他们更担心的不是长期危险，而是短期问题，比如有偏见的反应和不正确的反应。</p><p>但也有人认为，人工智能的进步实在太快，以至于它在某些领域的表现已经超过了人类的水平，而且很快就会在其他领域超越人类。他们说，这项技术已经显示出先进能力和先进理解力的迹象，这让人们担心，“通用人工智能”(AGI)——可以在各种任务中达到或超过人类水平的人工智能——可能已经离我们不远了。</p><p>在<a rel="noopener noreferrer" target="_blank" href="https://openai.com/blog/governance-of-superintelligence">上周的一篇博客文章</a>中，奥尔特曼和OpenAI的另外两名高管提出了几种负责任地管理强大人工智能系统的方法。他们呼吁领先的人工智能制造商进行合作，对大型语言模型进行更多技术研究，并成立一个类似于国际原子能机构（旨在控制核武器的使用）的国际人工智能安全组织。</p><p>奥尔特曼还表示，支持制定规则，要求大型尖端人工智能模型的制造者注册获得政府颁发的许可证。</p><p>今年3月，1000多名技术人员和研究人员<a href="https://cn.nytimes.com/technology/20230330/ai-artificial-intelligence-musk-risks/">签署了另一封公开信</a>，呼吁暂停开发最大的人工智能模型六个月，理由是担心“开发和部署更强大数字思维的竞赛失控”。</p><p>这封信由另一家专注于人工智能的非营利组织——生命未来研究所牵头，伊隆·马斯克和其他知名科技领袖签名，但来自领先人工智能实验室的签名并不多。</p><p>亨德里克斯说，人工智能安全中心这份简短的新声明——总共只有22个英文单词——旨在团结人工智能专家，这些专家可能对特定风险的性质或防止这些风险发生的措施存在分歧，但他们对强大的人工智能系统有着共同的担忧。</p><p>“我们不想推动一个包含30种潜在干预措施的大菜单，”他说。“这种事会稀释信息。”</p><p>这份声明最初被分享给几位知名人工智能专家，其中包括辛顿。辛顿本月<a href="https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html" title="Link: https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html">辞去了在谷歌的工作</a>，他说，这样他就可以更自由地谈论人工智能的潜在危害。之后，该声明进入了几个主要的人工智能实验室，然后一些员工在上面签名。</p><p>随着成百上千万人使用人工智能聊天机器人寻求娱乐、陪伴和提高工作效率，以及底层技术的快速进步，来自该领域领导者的警告的紧迫性也在增加。</p><p>“我认为，如果这项技术出了问题，可能会出大问题，”奥尔特曼告诉参议院小组委员会。“我们希望与政府合作，防止这种情况发生。”</p></section><footer><p>Kevin Roose是科技专栏作者，著有《Futureproof: 9 Rules for Humans in the Age of Automation》一书。欢迎在<a rel="nofollow" target="_blank" href="https://twitter.com/kevinroose">Twitter</a>和<a rel="nofollow" target="_blank" href="https://www.facebook.com/kevinroose">Facebook</a>上关注他。</p><p>翻译：晋其角</p><p><a rel="nofollow" target="_blank" href="https://www.nytimes.com/2023/05/30/technology/ai-threat-warning.html">点击查看本文英文版。</a></p></footer><br><hr><div>获取更多RSS：<br><a href="https://feedx.net" style="color:orange" target="_blank">https://feedx.net</a> <br><a href="https://feedx.best" style="color:orange" target="_blank">https://feedx.best</a><br></div>
