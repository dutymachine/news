<!--1683623222000-->
[人工智能带来的新恐惧：当自主杀人机器人成为军方工具](https://cn.nytimes.com/technology/20230509/ai-military-war-nuclear-weapons-russia-china/)
------

<address>DAVID E. SANGER</address><time pudate="2023-05-09 04:57:21" datetime="2023-05-09 04:57:21">2023年5月9日</time><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/05/05/multimedia/05dc-ai-wh1-ztlm/05dc-ai-wh1-ztlm-master1050.jpg" width="1050" height="700"><figcaption>“如果我们停下来，猜猜谁不会停下来：海外的潜在对手，”五角大楼的首席信息官约翰·舍曼周三表示。“我们必须继续前进。” <cite>Haiyun Jiang/The New York Times</cite></figcaption></figure><section><p>去年10月宣布严格限制向中国出售最先进的计算机芯片时，拜登总统给出的解释是，这一定程度上是为了给美国工业一个恢复竞争力的机会。</p><p>但在五角大楼和国家安全委员会，还有另外一个议程：军备控制。</p><p>从理论上说，如果中国军方拿不到芯片，可能会放慢研发人工智能武器的努力。这将让白宫和世界有时间制定出一些关于传感器、导弹和网络武器中使用人工智能的规则，终极目标则是防范好莱坞设想的那种噩梦——把自己的人类创造者关在门外的自主杀手机器人和计算机——变成现实。</p><p>现在，围绕着广受欢迎的ChatGPT聊天机器人和其他生成式人工智能软件的恐惧迷雾让<a href="https://www.nytimes.com/2022/10/13/us/politics/biden-china-technology-semiconductors.html">限制中国获取芯片</a>看起来只是一个权宜之计。上周四，拜登在白宫参加了一个科技高管会议，这些高管们正在想方设法限制这项技术的风险。拜登的第一句话是，“你们正在做的事情具有巨大的潜力和巨大的危险。”</p><p>他的国家安全助手表示，这是对最近有关这项新技术可能颠覆战争、网络冲突以及——在最极端的情况下，颠覆核武器使用决策的机密简报所做的反应。</p><p>但就在拜登发出警告的同时，五角大楼官员在科技论坛上表示，他们认为暂停开发下一代ChatGPT和类似软件六个月的想法是个坏主意：中国人不会等，俄罗斯人也不会等。</p><p>“如果我们停下来，猜猜谁不会停下来：海外的潜在对手，”五角大楼的首席信息官约翰·舍曼<a rel="noopener noreferrer" target="_blank" href="https://fcw.com/defense/2023/05/pentagon-cio-warns-against-pause-ai-development/385994/" title="Link: https://fcw.com/defense/2023/05/pentagon-cio-warns-against-pause-ai-development/385994/">上周三表示</a>。“我们必须继续前进。”</p><p>他的直白突显了当今整个国防界的紧张气氛。没有人真的清楚这些新技术在开发和控制武器方面的能力，而且也不了解什么样的军备控制制度（如果有的话）可能奏效。</p><p>这种预感虽然还不明确，但令人深感担忧。ChatGPT会让那些以前不容易接触到破坏性技术的坏人得逞吗？它是否会加速超级大国之间的对抗，以致没有时间进行外交和谈判？</p><p><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/05/05/multimedia/05dc-ai-weapons-jctf/05dc-ai-weapons-jctf-master1050.jpg"><small style="color: #999;">“行业现在正在进行一系列非正式对话——所有这些都是非正式的——探讨人工智能的规则会是什么样的，”曾担任国防创新委员会主席的前谷歌董事长施密特说。</small></p><p>“这个行业并不愚蠢，你已经看到了自我监管的努力，”谷歌前董事长埃里克·施密特说，他曾在2016年至2020年担任<a href="https://www.nytimes.com/2020/05/02/technology/eric-schmidt-pentagon-google.html">国防创新委员会</a>首任主席。</p><p>“因此，该行业现在正在进行一系列非正式对话——所有这些都是非正式的——探讨人工智能的规则会是什么样的，”施密特说，他与前国务卿亨利·基辛格合作撰写了<a rel="noopener noreferrer" target="_blank" href="https://www.foreignaffairs.com/reviews/capsule-review/2022-02-22/age-ai-and-our-human-future">一系列文章和书籍</a>，探讨人工智能颠覆地缘政治的潜力。</p><p>任何测试过最初几个版本的ChatGPT的人都看到了将保护措施置入系统的努力。例如，机器人不会回答有关如何自制药物伤害他人的问题，或者如何炸毁大坝或破坏核离心机，美国和其他国家在没有人工智能工具的情况下都参与过这些行动。</p><p>但是这类行为黑名单只能减缓对系统的滥用；很少有人认为他们可以完全阻止这种行为。总是有办法绕过安全限制，任何试图关闭汽车安全带警告系统紧急蜂鸣声的人都可以证明这一点。</p><p>尽管这个新软件让更多人看到了这个问题，但对于五角大楼来说，这并不是什么新鲜事。关于开发自主武器的第一条规则在十年前发布。五角大楼的联合人工智能中心成立于五年前，旨在探索人工智能在战斗中的应用。</p><p>一些武器已经处于自动巡航模式。击落进入受保护空域的导弹或飞机的爱国者导弹长期以来一直都有“自动”模式。这个模式使它们能够在没有人工干预的情况下被飞来的目标触发并开火，速度快于人类的反应。不过它们应该接受人类的监督，人类可以在必要时中止攻击。</p><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/05/05/multimedia/05dc-ai-wh2-wjhp/05dc-ai-wh2-wjhp-master1050.jpg" alt="击落进入受保护空域的导弹或飞机的爱国者导弹一直都有“自动”模式。不过它们应该是接受人类监督的，人类可以在必要时中止攻击。" data-src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/05/05/multimedia/05dc-ai-wh2-wjhp/05dc-ai-wh2-wjhp-master1050.jpg"><figcaption>击落进入受保护空域的导弹或飞机的爱国者导弹一直都有“自动”模式。不过它们应该是接受人类监督的，人类可以在必要时中止攻击。 <cite>Sean Murphy/Associated Press</cite></figcaption></figure><p>针对伊朗资深核科学家穆赫森·法赫里扎德的暗杀行动是由以色列的摩萨德使用<a href="https://www.nytimes.com/2021/09/18/world/middleeast/iran-nuclear-fakhrizadeh-assassination-israel.html">由人工智能辅助的自动机枪执行</a>的，尽管看起来存在很大程度的遥控操作。俄罗斯最近表示已经开始制造——但尚未部署——波塞冬核鱼雷。如果真的像俄罗斯鼓吹的那样，这种武器将能够自主穿越海洋，避开现有的导弹防御系统，在发射几天后投送核武器。</p><p>到目前为止，还没有针对此类自主武器的条约或国际协定。在这个时代，<a href="https://cn.nytimes.com/world/20230420/china-nuclear-weapons-russia-arms-treaties/">放弃军备控制协议的速度比谈判的速度还快</a>，达成这样的协议的希望渺茫。但ChatGPT及其同类产品提出的挑战类型不同，而且在某些方面更为复杂。</p><p>在军队中，注入人工智能的系统可以加快战场决策的速度，以至于它们会产生完全意料之外的打击风险，或者根据误导性或故意错误的攻击警报做出决策。</p><p>“在军事和国家安全领域，人工智能的一个核心问题是，你如何防御比人类决策速度更快的攻击，我认为这个问题尚未解决，”施密特说。“换句话说，导弹来得太快了，必须要有自动反应。但如果这是一个错误的信号怎么办？”</p><p>冷战期间充斥着错报的故事——本应用于练习核反应的训练磁带不知何故被输入了错误的系统，引发了苏联大规模进攻的警报。（是良好的判断力让人们没有惊慌失措。）新美国安全中心的保罗·沙雷在他2018年出版的《无人军队》一书中指出，“从1962年到2002年，至少发生了13起险些使用核武器的事件”，这“证实了这样一种观点，即未遂事件是核武器的正常情况，即使这很恐怖。”</p><p>出于这个原因，在超级大国之间的紧张局势远低于今天的时代，历任总统都试图通过谈判为各方的核决策留出更多时间，这样就没有人会匆忙卷入冲突。但是生成命令的AI有可能将各国往反方向推，加快决策速度。</p><p>好消息是，大国似乎会非常小心——因为他们知道对手的反应会是什么样子。但到目前为止，还没有经过商定的规则。</p><p>前国务院官员、赖斯、哈德利、盖茨和曼纽尔公司合伙人安雅·曼纽尔最近写道，中国和俄罗斯还没有准备好就人工智能进行军备控制谈判，但就这一主题进行会谈可以促进有关什么样的人工智能用途将被视为“过于危险”的讨论。</p><p>当然，五角大楼也会担心同意许多限制。</p><p>计算机科学家丹尼·希利斯是用于人工智能的并行计算机的先驱，他说，“我非常努力地争取制定一项政策，如果你有武器的自主元件，你需要一种关闭它们的方法。”希利斯也曾在国防创新委员会任职。他说，五角大楼的官员不同意，他们说，“如果我们能把它们关掉，敌人也能把它们关掉。”</p><p>更大的风险可能来自个人行为者、恐怖分子、勒索软件组织或拥有先进网络技术的小国——比如朝鲜——它们学会了如何克隆一个更小、限制更少的ChatGPT版本。他们可能会发现，生成式人工智能软件非常适合加速网络攻击和部署虚假信息。</p><p>负责微软信任与安全业务的汤姆·伯特最近在乔治·华盛顿大学的一个论坛上说，他认为人工智能系统将帮助防御者更快地发现异常行为，而不是帮助攻击者。<a href="https://www.nytimes.com/2023/04/07/technology/ai-chatbots-google-microsoft.html">微软正在加快使用这项新技术</a>改造其搜索引擎。有些专家不同意这一观点。但他表示，他担心人工智能可能会“加速”有针对性的虚假信息的传播。</p><p>所有这些都预示着军备控制新时代的到来。</p><p>一些专家说，既然不可能阻止ChatGPT和类似软件的传播，最理想的出路是限制推进这项技术所需的专用芯片和其他计算能力。毫无疑问，这将是未来几年提出的许多不同的军备控制计划之一，而看起来各核大国目前对旧武器的谈判至少可以说缺乏兴趣，更不用提新武器了。</p></section><footer><p>David E. Sanger是白宫和国家安全记者。他在时报任职38年，参与的三个团队获得了普利策奖，最近一次是在2017年获得国际报道奖。他最新出版的一本书是《完美武器：网络时代的战争、破坏和恐惧》(The Perfect Weapon: War, Sabotage and Fear in the Cyber Age)  。欢迎在<a rel="nofollow" target="_blank" href="https://twitter.com/SangerNYT">Twitter</a>和<a rel="nofollow" target="_blank" href="https://www.facebook.com/david.e.sanger">Facebook</a>上关注他。</p><p>翻译：纽约时报中文网</p><p><a rel="nofollow" target="_blank" href="https://www.nytimes.com/2023/05/05/us/politics/ai-military-war-nuclear-weapons-russia-china.html">点击查看本文英文版。</a></p></footer><br><hr><div>获取更多RSS：<br><a href="https://feedx.net" style="color:orange" target="_blank">https://feedx.net</a> <br><a href="https://feedx.best" style="color:orange" target="_blank">https://feedx.best</a><br></div>
