<!--1680163623000-->
[马斯克等千余名科技领袖呼吁暂停开发更先进人工智能](https://cn.nytimes.com/technology/20230330/ai-artificial-intelligence-musk-risks/)
------

<address>CADE METZ, GREGORY SCHMIDT</address><time pudate="2023-03-30 03:58:01" datetime="2023-03-30 03:58:01">2023年3月30日</time><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/03/29/multimedia/29musk-ai1-bjhm/29musk-ai1-bjhm-master1050.jpg" width="1050" height="700"><figcaption>Twitter和特斯拉的首席执行官埃隆·马斯克以及其他科技领袖批评称，开发更先进AI是一场“失控的竞赛”。 <cite>Benjamin Fanjoy/Associated Press</cite></figcaption></figure><section><p>包括伊隆·马斯克在内的1000多名科技界领袖和研究人员敦促人工智能实验室暂停对最先进系统的开发，并在一封<a rel="noopener noreferrer" target="_blank" href="https://futureoflife.org/open-letter/pause-giant-ai-experiments/">公开信</a>中警告，AI工具“对社会和人类构成深远风险”。</p><p>这封信由非营利组织生命未来研究所在周三发表，信中说，AI开发人员“陷入了一场失控的竞赛，开发和部署越来越强大的数字思维，以至于所有人——甚至包括它们的创造者——也无法理解、预测或可靠地控制它们”。</p><p>其他署名者包括苹果联合创始人史蒂夫·沃兹尼亚克，2020年总统参选人、企业家杨安泽(Andrew Yang)，以及负责设定世界“<a rel="nofollow" target="_blank">末日时钟</a>”的《原子科学家公报》的负责人蕾切尔·布朗森。</p><p>“这些东西正在塑造我们的世界，”长期以来批评AI系统缺陷的企业家、学者加里·马库斯在接受采访时说。“我们面临着不负责任的企业、广泛应用、监管缺失和大量未知因素引发的‘完美风暴’。”</p><p>ChatGPT、微软必应和谷歌的Bard等聊天机器人的背后都有AI，它们能够像人类一样进行对话，撰写各种主题的文章，执行诸如计算机编程等更复杂的任务。</p><p>开发更强大聊天机器人的推动力已经引发了一场竞赛，有可能决定科技行业接下来的领导者。但人们批评这些工具会在细节上出错，而且会传播虚假信息。</p><p>这封公开信呼吁暂停开发比<a href="https://cn.nytimes.com/technology/20230315/openai-gpt4-chatgpt/">GPT-4</a>更强大的AI系统，该聊天机器人由OpenAI于本月推出，马斯克为该研究实验室的联合创始人。信中说，暂停开发将为人工智能系统引入“共享安全协议”提供时间。信中还说，“如果无法快速实施这样的暂停，政府应该介入并制定暂停期。”</p><p>信中说，“只有当我们确信强大的AI系统将产生积极的影响，并且风险将是可控的时候”，才应该推进开发。</p><p>“人类可以享受AI带来的繁荣未来，”信中说。“在成功创建强大的AI系统之后，我们现在可以享受一个‘AI之夏’，在这个过程中，收获回报，设计监管系统，造福所有人，并给社会一个适应的机会。”</p><p>OpenAI的首席执行官萨姆·奥尔特曼没有在这封信上签名。</p><p>马库斯和其他人认为，说服更广泛的科技界同意暂停是很困难的。但政府迅速采取行动的可能性也很小，因为立法者在监管AI方面做得很少。</p><p>加州共和党众议员杰伊·奥伯诺尔特最近对《纽约时报》表示，美国政界对这项技术了解不多。2021年，欧盟政策制定者提出了一项法律，旨在规范可能造成伤害的AI技术，包括面部识别系统。</p><p>该措施预计最早将于今年通过，它要求公司对AI技术进行风险评估，以确定它们的应用将如何影响健康、安全和个人权利。</p><p>GPT-4就是AI研究人员所说的神经网络，一种通过分析数据来学习技能的数学系统。神经网络是Siri和Alexa等数字助理用来识别语音指令的技术，也是自动驾驶汽车用来识别行人的技术。</p><p>2018年前后，谷歌和OpenAI等公司开始构建从大量数字文本中学习的神经网络，这些数字文本包括书籍、维基百科文章、聊天记录和其他从互联网上挑选出来的信息。这些网络称为大型语言模型(LLM)。</p><p>通过在所有文本中找出数亿个模式，LLM学习自己生成文本，包括推文、学期论文和计算机程序。它们甚至可以进行对话。多年来，OpenAI和其他公司建立的LLM从越来越多的数据中学习。</p><p>这提高了它们的能力，但这些系统仍然会犯错。它们经常搞错事实，并且会在没有警告的情况下编造信息，这种现象被研究人员称为“幻觉”(hallucination)。由于系统似乎对其提供的所有信息完全自信，因此人们通常很难分辨什么是对的，什么是错的。</p><p>专家们担心，这些系统可能会遭到滥用，以比过去更快、更高效的方式传播虚假信息。他们认为，这些系统甚至会被人在网上拿来影响人们的行为。</p><p>在GPT-4发布之前，OpenAI要求外部研究人员测试该系统的危险用途。研究人员表明，它可以被诱使建议如何在网上购买非法枪支，描述如何用家居用品制造危险物质，以及写Facebook帖子来说服女性堕胎是不安全的。</p><p>他们还发现，该系统能够使用TaskRabbit在互联网上雇佣一个人并骗过验证码测试，该测试被广泛用于在线识别机器人。当人问系统是不是“机器人”时，系统自称视障人士。</p><p>经过OpenAI的修改后，GPT-4不再做这些事情了。</p><p>多年来，许多AI研究人员、学者和包括马斯克在内的技术高管都担心，AI系统可能造成更大的危害。其中一些人属于被称为理性主义者或有效利他主义者(effective altruists)的庞大网络群体，他们认为AI最终会毁灭人类。</p><p>这封信由生命未来研究所牵头，该组织致力于研究人类的生存风险，长期以来一直警告AI的危险。但签署者来自各行各业和学界人士。</p><p>尽管签署这封信的一些人以反复表达对AI可能毁灭人类的担忧而闻名，但包括马库斯在内的其他人更担心眼前的危险，包括虚假信息的传播，以及人们依赖这些系统进行医疗和情感咨询所导致的风险。</p><p>这封信“表明许多人都对正在发生的事情深感担忧”，签署这封信的马库斯说。他相信这封信将成为一个转折点。“我认为这是AI史上——也许也是人类史上——一个非常重要的时刻，”他说。</p><p>然而，他承认，那些签署这封信的人可能会发现，要说服更广泛的AI行业的公司和研究人员暂停很难。“这封信并不完美，”他说。“但传达的精神是完全正确的。”</p></section><footer><p>Cade Metz是一名科技记者，负责报道人工智能、无人驾驶汽车、机器人、虚拟现实和其他新兴领域。他曾为《连线》杂志撰稿。 欢迎在Twitter上关注他：<a rel="nofollow" target="_blank" href="https://twitter.com/cademetz">@cademetz</a>。</p><p>Gregory Schmidt报道突发新闻和房地产市场，他是<a rel="nofollow" target="_blank" href="https://www.nytimes.com/column/square-feet">Square Feet</a>专栏的编辑。欢迎爱Twitter上关注他：<a rel="nofollow" target="_blank" href="https://twitter.com/GregoryNYC">@GregoryNYC</a>。</p><p>翻译：明斋</p><p><a rel="nofollow" target="_blank" href="https://www.nytimes.com/2023/03/29/technology/ai-artificial-intelligence-musk-risks.html">点击查看本文英文版。</a></p></footer><br><hr><div>获取更多RSS：<br><a href="https://feedx.net" style="color:orange" target="_blank">https://feedx.net</a> <br><a href="https://feedx.best" style="color:orange" target="_blank">https://feedx.best</a><br></div>
