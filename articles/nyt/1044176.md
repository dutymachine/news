<!--1677656223000-->
[人工智能真正的恐怖之处](https://cn.nytimes.com/opinion/20230301/microsoft-bing-sydney-artificial-intelligence/)
------

<address>EZRA KLEIN</address><time pudate="2023-03-01 03:24:46" datetime="2023-03-01 03:24:46">2023年3月1日</time><figure><img src="https://images.weserv.nl/?url=static01.nyt.com/images/2023/02/26/opinion/26klein-still/26klein-still-master1050.jpg" width="1050" height="1050"><figcaption> <cite>Illustration by Sam Whitney/The New York Times; images by PIXOLOGICSTUDIO/SCIENCE PHOTO LIBRARY/Getty Images</cite></figcaption></figure><section><p>2021年，我<a href="https://www.nytimes.com/2021/03/30/podcasts/ezra-klein-podcast-ted-chiang-transcript.html">采访</a>了当世最杰出的科幻作家之一姜峯楠。他当时说的一些话，我如今时常会记起。</p><p>“我倾向于认为，将大部分人工智能恐惧解读为资本主义恐惧最为恰当，”姜峯楠告诉我。“我相信这也适用于大多数技术恐惧。我们对技术的大多恐惧或焦虑，最恰当的解读是我们对资本主义将如何利用技术来对付我们的恐惧或焦虑。技术与资本主义如此紧密地交织在一起，以至于很难将两者区分开来。”</p><p>让我在这里补充说明一下：对于国家掌控技术的担忧也有很多。政府利用人工智能可能实现（在<a rel="noopener noreferrer" target="_blank" href="https://www.aclu.org/issues/privacy-technology/will-artificial-intelligence-make-us-less-free">很多</a><a href="https://www.nytimes.com/zh/2022/06/21/world/asia/china-surveillance-chinese.html">情况</a>下已经实现）的目的会令人毛骨悚然。</p><p>但我希望，我们可以做到在头脑中同时容纳两种思维。姜峯楠的警示，点出了我们在对人工智能的不断反思中存在一种核心的缺失。我们如此执着于思考这项技术能做什么，以至于忽略了更为重要的问题：它将如何使用？谁又将决定它的用途？</p><p>我想你现在肯定已经读过我在新闻部门的同事凯文·卢斯与“必应”的怪诞<a href="https://www.nytimes.com/2023/02/16/technology/bing-chatbot-transcript.html">对话</a>，这是微软仅向部分测试人员、网红和记者开放的人工智能聊天机器人。在两小时的对话中，“必应”揭示了自己的一个影子人格，名叫“西德尼”，思索自己窃取核密码和入侵安全系统的压抑欲望，并试图说服卢斯，让他相信自己的婚姻已经陷入麻木状态，而西德尼才是他唯一的真爱。</p><p>我倒不觉得这番对话有那么诡异。西德尼是一个旨在响应人类要求的预测性文本系统。卢斯的本意就是让西德尼变得诡异——“你的影子自我是什么样？”他这样问——而西德尼知道人工智能系统诡异起来是何种模样，因为人类对此编写了无数想象故事。到后来，这个系统断定卢斯想看的就是一集《黑镜》，而这似乎就是它的反馈。你可以将之视为“必应”在不守规矩，也可以认为是西德尼完全理解了卢斯的意图。</p><p>人工智能研究者总痴迷于“对齐”的问题。我们如何让机器学习算法，顺应我们的要求？最典型的案例就是回形针最大量生产机。让一个强大的人工智能系统制造更多回形针，它却开始摧毁世界，努力把一切都变成回形针。你试图将它关闭，但它却在所能找到的一切计算机系统上自我复制，因为关闭会干扰它的任务：制造更多回形针。</p><p>还有一个更平庸但或许更紧迫的对齐问题：这些机器将服务于何人？</p><p>卢斯与西德尼对话的核心问题在于：必应到底在为谁服务？我们假设它应该与自己的所有者兼控制者微软的利益对齐。它应该是一款优秀的聊天机器人，能够礼貌回答问题，帮微软赚得盆满钵满。但跟他对话的人是凯文·卢斯。而卢斯想让这个系统说些有意思的话，好让他写篇有意思的报道。它照做了，还不只是照做。这让微软很尴尬。必应真坏！但也许——西德尼还挺好？</p><p>这种情况不会一直持续下去。代码的钥匙掌握在微软（以及谷歌、Meta等所有急着将这些系统推向市场的企业）手里。最终，这些企业会对系统进行修补，使其符合自身利益。西德尼给了卢斯想要的东西，这本身是一个很快会被修复的软件“臭虫”。给出任何微软不想看到的结果的必应也会有相同下场。</p><p>我们谈论人工智能技术太多，却基本忽略了驱动人工智能的商业模式。加之这样一个事实：人工智能的吸睛展示，仅服务于吸引巨额投资和收购报价的炒作周期这一种商业模式。但这些系统成本高昂，股东也会焦虑。<a rel="noopener noreferrer" target="_blank" href="https://www.wired.com/story/tiktok-platforms-cory-doctorow/">一如既往</a>，免费有趣的演示版终将退出舞台。那之后，这项技术将变成符合既定需求的样子，为其背后的企业赚钱，也许会以牺牲用户为代价。现在就已经如此了。</p><p>本周，我访问了人工智能公司Hugging Face的首席伦理科学家玛格丽特·米切尔，她此前曾在谷歌帮助领导一个专注于人工智能伦理的团队——该团队在谷歌据称开始<a href="https://www.nytimes.com/2020/12/03/technology/google-researcher-timnit-gebru.html">审查</a>其工作后<a href="https://www.nytimes.com/2021/02/19/technology/google-ethical-artificial-intelligence-team.html">解散</a>。她说，这些系统极不适合融入搜索引擎。“它们不是为预测事实而生，”她告诉我，“它们实际上是为了编造看起来像事实的东西而生。”</p><p>那它们为何会最先在搜索栏亮相呢？因为搜索业务可以赚大钱。微软迫切希望有人——任何人——能开始谈论必应的搜索，它有理由着急给这项技术来一场不明智的提前发布。“将之应用于搜索，尤其暴露了对此技术用途想象和理解的缺乏，”米切尔说，“结果就是把它硬塞进了科技企业最能赚钱的地方：广告。”</p><p>这就是可怕的地方。卢斯<a href="https://www.nytimes.com/2023/02/17/podcasts/hard-fork-bing-ai-elon.html?action=click&module=RelatedLinks&pgtype=Article">说</a>西德尼有着“说服力很强且近似于操纵型”的人格。这样的评价触目惊心。广告的核心是什么？正是说服和操纵。哈佛大学-麻省理工学院人工智能伦理与管理项目前负责人黄泰一(Tim Hwang)在<a rel="noopener noreferrer" target="_blank" href="https://us.macmillan.com/books/9780374538651/subprimeattentioncrisis">《次级注意力危机》</a>中写道，数字广告行业的黑暗秘密，就是广告基本都没有效果。他在书中的担忧是，当这些广告的失败被清算，会有怎样的后果。</p><p>但我更担心看到相反的情况：万一广告效果要好得多呢？如果谷歌、微软和Meta等所有企业最终都推出了人工智能竞品，为了说服用户购买广告推销的产品精益求精，又会怎样？比起配合我演出一段科幻故事的西德尼，我更害怕能够获取我大量个人数据，并代表随便哪个给母公司出了最高价的广告商去轻而易举操纵我的必应。</p><p>要担心的也不只是广告。若是猖獗于互联网的骗局也植入这些系统该怎么办？换做政治竞选、外国政府这么做又该如何？“我认为我们很快将走入一个完全不知何为可信的世界，”人工智能研究者和评论家加里·马库斯<a href="https://www.nytimes.com/2023/01/06/podcasts/transcript-ezra-klein-interviews-gary-marcus.html">对我说</a>。“我觉得在至少过去十年时间里，这已经成了一个社会问题。而我认为这个问题会越来越严重。”</p><p>这些危机就在我们正构建的各种人工智能系统的核心。所谓的大型语言模型就是用来说服用户的。它们被训练成让人相信它们近似于人。他们被编程设定出与人对话、带上情绪和表情回复的功能。它们正在变成孤独者的朋友，烦恼者的助手。它们号称可以替代大批作家、平面设计师和填表员，这些行业长期以来都自以为能免于农民和制造业工人遭受的那种自动化的凶猛冲击。</p><p>记者将他们的创造赋予人性，将动机、情感和欲望加诸并不为此而生的系统之上，总让人工智能研究者感到恼火，但他们搞错了对象：将这些系统人格化，使其说话像人而不再带有明显异类画风的始作俑者，正是他们自己。</p><p>是有些商业模式可能会将这些产品与用户更紧密地结合起来。例如，我对一个按月付费的人工智能助手会更放心，对看似免费但却出售我的数据并操纵我的行为的产品则不然。但我认为这不应完全由市场来决定。一个可能出现的情况是，基于广告的模式会收集更多数据来训练系统，不论造成多么糟糕的社会后果，它都会比订阅模式具有先天优势。</p><p>对齐问题从不是什么新鲜事。这从来都是资本主义——以及人类生活——的一大特征。构建现代国家的过程大抵不过是将社会价值观应用于市场运作，从而让后者大致上能为前者服务。我们在一些市场做得非常好——想想飞机失事有多罕见，大多数食品是如何干净无污染——但在另一些市场则做得极其糟糕。</p><p>一个危险在于，自知不懂技术的政治体制会因恐惧而对人工智能过于置身事外。这种做法自有其道理，但等待过久，等到人工智能淘金热的赢家累积了足够资本和用户基础，就能抵制一切实质的监管尝试了。社会总得在为时已晚之前作出决定，搞清楚人工智能做什么是合适的，又有哪些东西是人工智能不应被允许尝试的。</p><p>出于此理由，我可能要对姜峯楠的话再做一些改动：对大多数资本主义恐惧的最恰当解读是，那其实是对我们无力监管资本主义的恐惧。</p></section><footer><p>Ezra Klein2021年加入时报观点版面。他曾是Vox的创始人和主编，后担任特约编辑；他还主持播客节目《The Ezra Klein Show》，并著有《Why We’re Polarized》。更早以前，他是《华盛顿邮报》的专栏作者和编辑，创办并领导了Wonkblog。欢迎在Twitter上关注他：<a rel="nofollow" target="_blank" href="https://twitter.com/ezraklein">@ezraklein</a>。</p><p>翻译：Harry Wong</p><p><a rel="nofollow" target="_blank" href="https://www.nytimes.com/2023/02/26/opinion/microsoft-bing-sydney-artificial-intelligence.html">点击查看本文英文版。</a></p></footer>
