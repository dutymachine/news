<!--1689993545000-->
[人工智能公司与白宫就安全准则达成协议](https://www.voachinese.com/a/ai-firms-strike-deal-with-white-house-for-safety-guidelines-20230721/7191588.html)
------

<div><i>Sat, 22 Jul 2023 02:09:08 GMT</i></div><img src="https://images.weserv.nl?url=gdb.voanews.com/01000000-0a00-0242-cd64-08db8a4896d9_r1_s_w900.jpg" width="100%"><div><small style="color: #999;">2023年7月21日，拜登总统在白宫罗斯福厅就人工智能发表讲话。聆听者左起为亚马逊网络服务首席执行官亚当·塞利普斯基、 OpenAI 总裁格雷格·布罗克曼、Meta总裁尼克·克莱格、Inflection AI首席执行官穆斯塔法·苏莱曼。（美联社照片）</small></div><p>白宫周五（7月21日）宣布，拜登行政当局已与七家制造人工智能产品的公司达成自愿协议，以制定旨在确保该技术安全开发的指导方针。<br /><br />“这些承诺是真实的，而且是具体的，”乔·拜登（Joe Biden）总统周五下午在对媒体的谈话中说。“他们会帮忙......该行业履行其对美国人民的基本义务，即安全开发、可靠和值得信赖的技术，而该技术能造福社会并维护我们的价值观和我们共同的价值观。”<br /><br />周五将企业领导人派往白宫的公司是亚马逊（Amazon）、人择（Anthropic）、谷歌（Google）、Inflection、Meta、微软（Microsoft）和OpenAI。这些公司都在开发叫做大型语言模型（LLM）的系统，该系统使用大量的文本进行训练，这些文本通常取自可公开访问的互联网，并使用预测分析来对话式地响应查询。<br /><br />创建流行的ChatGPT服务的OpenAI公司在一份声明中表示，“这一过程由白宫协调，是在美国和世界各地推进有意义和有效的人工智能治理的重要一步。”<br /><br /><strong>安全、安保、信任</strong><br /><br />白宫周五上午发布的该协议概述了三大重点领域：确保人工智能产品在广泛可用之前让公众可以安全使用，构建安全且不能被滥用于意外目的的产品，以及建立公众对开发该技术的公司的信任，以及它们的工作方式和收集的信息是透明的。<br /><br />作为协议的一部分，各公司承诺在人工智能系统公开之前进行内部和外部安全测试，以确保它们可以让公众安全使用，并与公众分享有关安全和安保的信息。<br /><br />此外，该承诺要求公司保持强有力的安全保障措施，以防止无意或恶意发布不能面向公众的技术和工具，并支持第三方检测和揭露任何此类违规行为的努力。<br /><br />再者，该协议规定了旨在建立公众信任的一系列义务。其中包括保证人工智能生成的内容将如此标明；公司将提供有关其产品功能和局限性的明确信息；公司将优先考虑降低人工智能潜在危害的风险，包括偏见、歧视和侵犯隐私；最后，公司将把研究重点放在使用人工智能来“帮助解决社会面临的最重大挑战”上。<br /><br />拜登政府表示，它正在制定一项行政命令，要求国会制定立法，以“帮助美国在负责任的创新方面处于领先地位”。<br /><br /><strong>只是刚刚开始</strong><br /><br />美国之音(VOA)联系的专家都表示，周五宣布的协议标志着在有效监管新兴人工智能技术的道路上迈出了积极的一步，但他们也警告说，在了解这些强大的模型可能造成的潜在危害和找到减缓危害的方法方面，还有更多的工作要做。<br /><br />“没有人知道如何监管人工智能——它非常复杂，而且在不断变化，”乔治·华盛顿大学（George Washington University）教授、数字贸易和数据治理中心的创始人兼主任苏珊·阿里尔·亚伦森（Susan Ariel Aaronson）说。<br /><br />“白宫正在努力以有利于创新的方式进行监管，”阿伦森对美国之音说。“当你进行监管时，你总是希望平衡风险——保护个人或企业免受伤害——同时鼓励创新，而这个行业对美国经济增长至关重要。”<br /><br />她补充说：“美国正在努力，所以我想赞扬白宫的这些努力。但我想说实话。目前做得够吗？不，还不够。”<br /><br /><strong>对话式计算</strong><br /><br />Unanimous AI公司的首席执行官兼首席科学家路易斯·罗森伯格（Louis Rosenberg）说，重要的是要做对这一点：因为像ChatGPT、谷歌的Bard和人择的Claude这样的模型，将越来越多地被内置到人们用来进行日常业务的系统中。<br /><br />“我们正在进入一个对话式计算的时代，我们将与电脑交谈，我们的电脑将会回话，”罗森伯格对美国之音说。“这就是我们将如何与搜索引擎接触的方式。这就是我们将如何与应用程序接触的方式。这就是我们将如何利用生产力工具的方式。”<br /><br />罗森伯格在人工智能领域工作了30年，拥有数百项相关专利，他说，当谈到“大型语言模型”与我们的日常生活如此紧密地融合在一起时，我们甚至不知道我们应该担心的到底是什么。<br /><br />“许多风险尚未被完全了解，”他说。他还表示，传统的计算机软件是非常确定性的，这意味着程序的构建正是为了做程序员告诉他们做的事情。相比之下，大型语言模型的确切操作方式甚至对其创建者来说都是不透明的。<br /><br />这些模型可能会表现出意想不到的偏见，可以鹦鹉学舌虚假或误导性的信息，并且可以说出人们认为令人反感甚至危险的事情。此外，许多人将通过第三方服务（例如网站）与他们进行互动，该服务将大型语言模型集成到其产品中，但能够以可能是恶意或操纵的方式定制其回应内容。<br /><br />其中许多问题只有在大规模部署这些系统时才会变得明显，但到此时它们已经被公众使用了。<br /><br />“这些问题尚未浮出水面，因此政策制定者还无法迎头解决这些问题，”罗森伯格说。“我认为，积极的一面是，至少政策制定者已经预料到这些问题了。”<br /><br /><strong>需要更多利益相关者</strong><br /><br />兰德公司（RAND Corporation）政策分析师本杰明·布德罗（Benjamin Boudreaux）对美国之音说，目前尚不清楚周五的协议会对公司的行为产生多大的实际变化。<br /><br />“公司在这里同意的许多事情都是公司已经做的事情，所以目前尚不清楚这项协议是否真的改变了他们的大部分行为，” 布德罗说。“因此，我认为国会和白宫仍然需要采取更多的监管方法或采取更多行动。”<br /><br />布德罗还表示，随着行政当局不断充实其政策内容，它将不得不扩大对话参与者的范围。<br /><br />“这次只是一组私营部门实体；没有包括需要参与有关这些系统风险讨论的全套的利益相关者，”他说。“被排除在外的利益相关者包括一些独立评估者、公民社会组织、非营利组织等，它们实际上会进行一些风险分析和风险评估。”</p><br><hr><div>获取更多RSS：<br><a href="https://feedx.net" style="color:orange" target="_blank">https://feedx.net</a> <br><a href="https://feedx.best" style="color:orange" target="_blank">https://feedx.best</a><br></div>
